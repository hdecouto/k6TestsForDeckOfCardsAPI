Deck of Cards API – Performance Test Plan (k6 + TypeScript)
# 1) Goal and scope
## Goal
Evaluate the performance characteristics of the Deck of Cards API under representative traffic patterns, focusing on:
### Error rate
### Throughput/capacity
### Basic stability under sustained load
## Scope
This work sample is intentionally time-boxed and representative rather than exhaustive.
### In scope
#### Core API workflows (shuffle → draw)
#### Load patterns: baseline, stress, short soak
#### Metrics/thresholds and clear pass/fail signals (k6 thresholds)
### Out of scope
#### Production-like distributed load generation
#### Full endurance testing (hours)
#### Deep profiling / server-side instrumentation
### System under test
#### Deck of Cards API (local run). 
#### GitHub

# 2) Test approach
Method
Risk-based, workflow-oriented performance testing:
Identify core user/API workflows.
Establish a baseline with stable load.
Increase load to observe saturation and failure modes.
Run a short soak to look for degradation over time.
Why these metrics
Primary metrics: request rate, failure rate, and duration. 
Grafana Labs
# 3) Key API workflows and endpoints
Primary workflow used in scenarios:
Create + shuffle a new deck
/api/deck/new/shuffle/
Draw cards from the deck
/api/deck/{deck_id}/draw/?count=N
Optional: reshuffle existing deck
/api/deck/{deck_id}/shuffle/
Pulled from deckofcardsapi.com
# 4) Performance scenarios (scenario matrix)
Scenario	Purpose	Pattern	Example Load Shape	Success Criteria
Smoke (perf sanity)	Validate scripts + basic latency/error	Very low load	1–2 VUs, 1–2 min	0 unexpected failures; latency reasonable
Baseline (steady state)	Measure stable performance under expected traffic	Constant arrival/steady VUs	e.g., 10–25 VUs for 5–10 min	Thresholds met
Stress / ramp	Find saturation point and how it fails	Step or ramp-up	e.g., 10→50→100 VUs	Identify where error/latency inflects
Short soak	Detect degradation (GC, leaks, resource exhaustion)	Sustained	e.g., baseline load for 10–15 min	No upward latency trend; failures stay bounded
Notes:
“Expected traffic” is unknown, so baseline load is chosen to be moderate and repeatable. With more information I can calibrate the baseline to product expectations.
# 5) Measurement and evaluation
Metrics to track
Error rate: http_req_failed
Latency: http_req_duration (report avg/med and percentiles like p95/p99)
Throughput: http_reqs and/or iterations
start with request, error, and duration metrics. 

Percentiles
Latency (p95) is used because it reflects “most users” experience and catches long-tail slow requests. 
Better Stack
# 6) Pass/fail thresholds (k6)
k6 supports thresholds as first-class pass/fail checks (including http_req_failed and p95 latency on http_req_duration). 
Grafana Labs
Suggested default thresholds (tune during interview if needed):
http_req_failed: rate < 0.01 ( <1% )
http_req_duration: p(95) < 500ms (or 750ms if local dev env is slower)
Optional: http_req_duration: p(99) < 1000ms
Rationale: conservative, easy-to-explain defaults for a small API; thresholds are intended to be adjusted to SLOs when business targets are known.
# 7) Test data and realism assumptions
Each VU maintains its own deck_id to avoid cross-user interference (closer to real usage).
Draw sizes: mix of count=1, count=5, count=10 to emulate typical usage patterns.
Think time: small randomized sleep between operations to avoid unrealistically tight loops.